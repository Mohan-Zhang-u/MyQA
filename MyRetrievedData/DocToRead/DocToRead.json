{"data": [{"title": "", "paragraphs": [{"context": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.\n\n\nContents\n1\tOverview\n2\tHistory\n3\tArchitecture\n4\tQuestion answering methods\n4.1\tOpen domain question answering\n5\tIssues\n6\tProgress\n7\tReferences\n8\tFurther reading\n9\tExternal links\nOverview\nA QA implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.\n\nSome examples of natural language document collections used for QA systems include:\n\na local collection of reference texts\ninternal organization documents and web pages\ncompiled newswire reports\na set of Wikipedia pages\na subset of World Wide Web pages\nQA research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions.\n\nClosed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease[1]\nOpen-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\nHistory\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2016) (Learn how and when to remove this template message)\nTwo early QA systems were BASEBALL[2] and LUNAR.[3] BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.\n\nSHRDLU was a highly successful question-answering program developed by Terry Winograd in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the \\\"blocks world\\\"), and it offered the possibility of asking the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\n\nIn the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.\n\nThe 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\n\nRecently, specialized natural language QA systems have been developed, such as EAGLi for health and life scientists.\n\nArchitecture\nAs of 2001, QA systems typically included a question classifier module that determines the type of question and the type of answer.[4] A multiagent question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta\u2013agent controls the cooperation between question answering agents and chooses the most relevant answer(s).[5]\n\nQuestion answering methods\nQA is very dependent on a good search corpus - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,[6] leading to two benefits:\n\nBy having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.\nCorrect answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\nSome question answering systems rely heavily on automated reasoning.[7][8] There are a number of question answering systems designed in Prolog,[9] a logic programming language associated with artificial intelligence.\n\nOpen domain question answering\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2016) (Learn how and when to remove this template message)\nIn information retrieval, an open domain question answering system aims at returning an answer in response to the user\\'s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from computational linguistics, information retrieval and knowledge representation for finding answers.\n\nThe system takes a natural language question as an input rather than a set of keywords, for example, \\\"When is the national day of China?\\\" The sentence is then transformed into a query through its logical form. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\n\nKeyword extraction is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. \\\"Who\\\", \\\"Where\\\" or \\\"How many\\\", these words tell the system that the answers should be of type \\\"Person\\\", \\\"Location\\\", \\\"Number\\\" respectively. In the example above, the word \\\"When\\\" indicates that the answer should be of type \\\"Date\\\". POS (Part of Speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is \\\"Chinese National Day\\\", the predicate is \\\"is\\\" and the adverbial modifier is \\\"when\\\", therefore the answer type is \\\"Date\\\". Unfortunately, some interrogative words like \\\"Which\\\", \\\"What\\\" or \\\"How\\\" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context.\n\nOnce the question type has been identified, an Information retrieval system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as \\\"Who\\\" or \\\"Where\\\", a Named Entity Recogniser is used to find relevant \\\"Person\\\" and \\\"Location\\\" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\n\nA vector space model can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is \\\"1st Oct.\\\"\n\nIssues\nIn 2002, a group of researchers presented an unpublished and largely unsourced report as a funding support document, in which they describe a 5-year roadmap of research current to the state of the question answering field at that time.\n\nProgress\nQA systems have been extended in recent years to encompass additional domains of knowledge[10] For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:\n\ninteractivity\u2014clarification of questions or answers\nanswer reuse or caching\nanswer presentation[11]\nknowledge representation and reasoning\nsocial media analysis with QA systems\nsentiment analysis[12]\nutilization of thematic roles[13]\nsemantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts[14]\nutilization of linguistic resources,[15] such as WordNet, FrameNet, and the similar\nIBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. [16] Facebook Research has made their DrQA system[17] available under an open source license. This system has been used for open domain question answering using Wikipedia as knowledge source[18].\n\nNatural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n\nContents\n1\tHistory\n2\tRule-based vs. statistical NLP\n3\tMajor evaluations and tasks\n3.1\tSyntax\n3.2\tSemantics\n3.3\tDiscourse\n3.4\tSpeech\n4\tSee also\n5\tReferences\n6\tFurther reading\n7\tExternal links\nHistory\nThe history of natural language processing generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \\\"Intelligence\\\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \\\"blocks worlds\\\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \\\"patient\\\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \\\"My head hurts\\\" with \\\"Why do you say your head hurts?\\\".\n\nDuring the 1970s, many programmers began to write \\\"conceptual ontologies\\\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore\\'s law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\n\nIn the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that were used in statistical machine translation (SMT).\n\nRule-based vs. statistical NLP\nIn the early days, many language-processing systems were designed by hand-coding a set of rules,[9][10], e.g. by writing grammars or devising heuristic rules for stemming. However, this is rarely robust to natural language variation.\n\nSince the so-called \\\"statistical revolution\\\"[11][12] in the late 1980s and mid 1990s, much natural language processing research has relied heavily on machine learning.\n\nThe machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, \\\"corpora\\\") is a set of documents, possibly with human or computer annotations).\n\nMany different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \\\"features\\\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n\nSystems based on machine-learning algorithms have many advantages over hand-produced rules:\n\nThe learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\nAutomatic learning procedures can make use of statistical-inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with hand-written rules\u2014or, more generally, creating systems of hand-written rules that make soft decisions\u2014is extremely difficult, error-prone and time-consuming.\nSystems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.\nMajor evaluations and tasks\nThe following is a list of some of the most commonly researched tasks in natural language processing. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n\nThough natural language processing tasks are closely intertwined, they are frequently subdivided into categories for convenience. A coarse division is given below.\n\nSyntax\nGrammar induction[13]\nGenerate a formal grammar that describes a language\\'s syntax.\nLemmatization\nThe task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma.\nMorphological segmentation\nSeparate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. \\\"open, opens, opened, opening\\\") as separate words. In languages such as Turkish or Meitei,[14] a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\nPart-of-speech tagging\nGiven a sentence, determine the part of speech for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \\\"book\\\" can be a noun (\\\"the book on the table\\\") or verb (\\\"to book a flight\\\"); \\\"set\\\" can be a noun, verb or adjective; and \\\"out\\\" can be any of at least five different parts of speech. Some languages have more such ambiguity than others.[dubious \u2013 discuss] Languages with little inflectional morphology, such as English, are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.\nParsing\n(see also: Stochastic grammar) Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses. In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing, Dependency Parsing and Constituency Parsing. Dependency Parsing focuses on the relationships between words in a sentence (marking things like Primary Objects and predicates), whereas Constituency Parsing focuses on building out the Parse Tree using a Probabilistic Context-Free Grammar (PCFG).\nSentence breaking (also known as sentence boundary disambiguation)\nGiven a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).\nStemming\nWord segmentation\nSeparate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like Bag of Words (BOW) creation in data mining.\nTerminology extraction\nThe goal of terminology extraction is to automatically extract relevant terms from a given corpus.\nSemantics\nLexical semantics\nWhat is the computational meaning of individual words in context?\nMachine translation\nAutomatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \\\"AI-complete\\\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly.\nNamed entity recognition (NER)\nGiven a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Note that, although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient. For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.\nNatural language generation\nConvert information from computer databases or semantic intents into readable human language.\nNatural language understanding\nConvert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[15]\nOptical character recognition (OCR)\nGiven an image representing printed text, determine the corresponding text.\nQuestion answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \\\"What is the capital of Canada?\\\"), but sometimes open-ended questions are also considered (such as \\\"What is the meaning of life?\\\"). Recent works have looked at even more complex questions.[16]\nRecognizing Textual entailment\nGiven two text fragments, determine if one being true entails the other, entails the other\\'s negation, or allows the other to be either true or false.[17]\nRelationship extraction\nGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\nSentiment analysis (see also multimodal sentiment analysis)\nExtract subjective information usually from a set of documents, often using online reviews to determine \\\"polarity\\\" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.\nTopic segmentation and recognition\nGiven a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.\nWord sense disambiguation\nMany words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.\nDiscourse\nAutomatic summarization\nProduce a readable summary of a chunk of text. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.\nCoreference resolution\nGiven a sentence or larger chunk of text, determine which words (\\\"mentions\\\") refer to the same objects (\\\"entities\\\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \\\"bridging relationships\\\" involving referring expressions. For example, in a sentence such as \\\"He entered John\\'s house through the front door\\\", \\\"the front door\\\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John\\'s house (rather than of some other structure that might also be referred to).\nDiscourse analysis\nThis rubric includes a number of related tasks. One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).\nSpeech\nSpeech recognition\nGiven a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \\\"AI-complete\\\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). Note also that in most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.\nSpeech segmentation\nGiven a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it.\nText-to-speech\nGiven a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired\n\nComputational linguistics\nFrom Wikipedia, the free encyclopedia\nJump to navigationJump to search\nThis article is about the scientific field. For the journal, see Computational Linguistics (journal).\nPart of a series on\nLinguistics\nOutlineHistoryIndex\nSubfields[hide]\nAcquisition Anthropological Applied Computational Discourse analysis Forensic Historical Lexicography Morphology Neurolinguistics Philosophy of language Phonetics Phonology Pragmatics Psycholinguistics Semantics Sociolinguistics Syntax\nGrammatical Theories[hide]\nCognitive Constraint-based Dependency Functional Generative Stochastic\nTopics[hide]\nDescriptivism Etymology Internet linguistics LGBT linguistics Linguistic anthropology Origin of language Origin of speech Orthography Prescriptivism Second-language acquisition Structuralism\nLinguistics portal\nvte\nComputational linguistics is an interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.\n\nTraditionally, computational linguistics was performed by computer scientists who had specialized in the application of computers to the processing of a natural language. Today, computational linguists often work as members of interdisciplinary teams, which can include regular linguists, experts in the target language, and computer scientists. In general, computational linguistics draws upon the involvement of linguists, computer scientists, experts in artificial intelligence, mathematicians, logicians, philosophers, cognitive scientists, cognitive psychologists, psycholinguists, anthropologists and neuroscientists, among others.\n\nComputational linguistics has theoretical and applied components. Theoretical computational linguistics focuses on issues in theoretical linguistics and cognitive science, and applied computational linguistics focuses on the practical outcome of modeling human language use.[1]\n\nThe Association for Computational Linguistics defines computational linguistics as:\n\n...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena.[2]\n\n\nContents\n1\tOrigins\n2\tApproaches\n2.1\tDevelopmental approaches\n2.2\tStructural approaches\n2.3\tProduction approaches\n2.3.1\tText-based interactive approach\n2.3.2\tSpeech-based interactive approach\n2.4\tComprehension approaches\n3\tApplications\n4\tSubfields\n5\tLegacy\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nOrigins\nComputational linguistics is often grouped within the field of artificial intelligence, but actually was present before the development of artificial intelligence. Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English.[3] Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language.[4] Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families. Earlier methods such as lexicostatistics and glottochronology have been proven to be premature and inaccurate. However, recent interdisciplinary studies which borrow concepts from biological studies, especially gene mapping, have proved to produce more sophisticated analytical tools and more trustworthy results.[5]\n\nWhen machine translation (also known as mechanical translation) failed to yield accurate translations right away, automated processing of human languages was recognized as far more complex than had originally been assumed. Computational linguistics was born as the name of the new field of study devoted to developing algorithms and software for intelligently processing language data. The term \\\"computational linguistics\\\" itself was first coined by David Hays, founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics.[6] When artificial intelligence came into existence in the 1960s, the field of computational linguistics became that sub-division of artificial intelligence dealing with human-level comprehension and production of natural languages.[citation needed]\n\nIn order to translate one language into another, it was observed that one had to understand the grammar of both languages, including both morphology (the grammar of word forms) and syntax (the grammar of sentence structure). In order to understand syntax, one had to also understand the semantics and the lexicon (or \\'vocabulary\\'), and even something of the pragmatics of language use. Thus, what started as an effort to translate between languages evolved into an entire discipline devoted to understanding how to represent and process natural languages using computers.[7]\n\nNowadays research within the scope of computational linguistics is done at computational linguistics departments,[8] computational linguistics laboratories,[9] computer science departments,[10] and linguistics departments.[11][12] Some research in the field of computational linguistics aims to create working speech or text processing systems while others aim to create a system allowing human-machine interaction. Programs meant for human-machine communication are called conversational agents.[13]\n\nApproaches\nJust as computational linguistics can be performed by experts in a variety of fields and through a wide assortment of departments, so too can the research fields broach a diverse range of topics. The following sections discuss some of the literature available across the entire field broken into four main area of discourse: developmental linguistics, structural linguistics, linguistic production, and linguistic comprehension.\n\nDevelopmental approaches\nLanguage is a cognitive skill which develops throughout the life of an individual. This developmental process has been examined using a number of techniques, and a computational approach is one of them. Human language development does provide some constraints which make it harder to apply a computational method to understanding it. For instance, during language acquisition, human children are largely only exposed to positive evidence.[14] This means that during the linguistic development of an individual, only evidence for what is a correct form is provided, and not evidence for what is not correct. This is insufficient information for a simple hypothesis testing procedure for information as complex as language,[15] and so provides certain boundaries for a computational approach to modeling language development and acquisition in an individual.\n\nAttempts have been made to model the developmental process of language acquisition in children from a computational angle, leading to both statistical grammars and connectionist models.[16] Work in this realm has also been proposed as a method to explain the evolution of language through history. Using models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span.[17] This was simultaneously posed as a reason for the long developmental period of human children.[17] Both conclusions were drawn because of the strength of the artificial neural network which the project created.\n\nThe ability of infants to develop language has also been modeled using robots[18] in order to test linguistic theories. Enabled to learn as children might, a model was created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development. It is important to note that this information could only have been empirically tested using a computational approach.\n\nAs our understanding of the linguistic development of an individual within a lifetime is continually improved using neural networks and learning robotic systems, it is also important to keep in mind that languages themselves change and develop through time. Computational approaches to understanding this phenomenon have unearthed very interesting information. Using the Price Equation and P\u00f3lya urn dynamics, researchers have created a system which not only predicts future linguistic evolution, but also gives insight into the evolutionary history of modern-day languages.[19] This modeling effort achieved, through computational linguistics, what would otherwise have been impossible.\n\nIt is clear that the understanding of linguistic development in humans as well as throughout evolutionary time has been fantastically improved because of advances in computational linguistics. The ability to model and modify systems at will affords science an ethical method of testing hypotheses that would otherwise be intractable.\n\nStructural approaches\nIn order to create better computational models of language, an understanding of language\u2019s structure is crucial. To this end, the English language has been meticulously studied using computational approaches to better understand how the language works on a structural level. One of the most important pieces of being able to study linguistic structure is the availability of large linguistic corpora, or samples. This grants computational linguists the raw data necessary to run their models and gain a better understanding of the underlying structures present in the vast amount of data which is contained in any single language. One of the most cited English linguistic corpora is the Penn Treebank.[20] Derived from widely-different sources, such as IBM computer manuals and transcribed telephone conversations, this corpus contains over 4.5 million words of American English. This corpus has been primarily annotated using part-of-speech tagging and syntactic bracketing and has yielded substantial empirical observations related to language structure.[21]\n\nTheoretical approaches to the structure of languages have also been developed. These works allow computational linguistics to have a framework within which to work out hypotheses that will further the understanding of the language in a myriad of ways. One of the original theoretical theses on internalization of grammar and structure of language proposed two types of models.[15] In these models, rules or patterns learned increase in strength with the frequency of their encounter.[15] The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck?[15] Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field.\n\nStructural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances.[22] For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances.[22] This technique is a strong tool for further probing the structure of human discourse. Without the computational approach to this question, the vastly complex information present in discourse data would have remained inaccessible to scientists.\n\nInformation regarding the structural data of a language is available for English as well as other languages, such as Japanese.[23] Using computational methods, Japanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length.[23] Though the exact cause of this lognormality remains unknown, it is precisely this sort of intriguing information which computational linguistics is designed to uncover. This information could lead to further important discoveries regarding the underlying structure of Japanese, and could have any number of effects on the understanding of Japanese as a language. Computational linguistics allows for very exciting additions to the scientific knowledge base to happen quickly and with very little room for doubt.\n\nWithout a computational approach to the structure of linguistic data, much of the information that is available now would still be hidden under the vastness of data within any single language. Computational linguistics allows scientists to parse huge amounts of data reliably and efficiently, creating the possibility for discoveries unlike any seen in most other approaches.\n\nProduction approaches\n\nThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (October 2015) (Learn how and when to remove this template message)\nThe production of language is equally as complex in the information it provides and the necessary skills which a fluent producer must have. That is to say, comprehension is only half the problem of communication. The other half is how a system produces language, and computational linguistics has made some very interesting discoveries in this area.\n\n\nAlan Turing: computer scientist and namesake developer of the Turing Test as a method of measuring the intelligence of a machine.\nIn a now famous paper published in 1950 Alan Turing proposed the possibility that machines might one day have the ability to \\\"think\\\". As a thought experiment for what might define the concept of thought in machines, he proposed an \\\"imitation test\\\" in which a human subject has two text-only conversations, one with a fellow human and another with a machine attempting to respond like a human. Turing proposes that if the subject cannot tell the difference between the human and the machine, it may be concluded that the machine is capable of thought.[24] Today this test is known as the Turing test and it remains an influential idea in the area of artificial intelligence.\n\n\nJoseph Weizenbaum: former MIT professor and computer scientist who developed ELIZA, a primitive computer program utilizing natural language processing.\nOne of the earliest and best known examples of a computer program designed to converse naturally with humans is the ELIZA program developed by Joseph Weizenbaum at MIT in 1966. The program emulated a Rogerian psychotherapist when responding to written statements and questions posed by a user. It appeared capable of understanding what was said to it and responding intelligently, but in truth it simply followed a pattern matching routine that relied on only understanding a few keywords in each sentence. Its responses were generated by recombining the unknown parts of the sentence around properly translated versions of the known words. For example, in the phrase \\\"It seems that you hate me\\\" ELIZA understands \\\"you\\\" and \\\"me\\\" which matches the general pattern \\\"you [some words] me\\\", allowing ELIZA to update the words \\\"you\\\" and \\\"me\\\" to \\\"I\\\" and \\\"you\\\" and replying \\\"What makes you think I hate you?\\\". In this example ELIZA has no understanding of the word \\\"hate\\\", but it is not required for a logical response in the context of this type of psychotherapy.[25]\n\nSome projects are still trying to solve the problem which first started computational linguistics off as its own field in the first place. However, the methods have become more refined and clever, and consequently the results generated by computational linguists have become more enlightening. In an effort to improve computer translation, several models have been compared, including hidden Markov models, smoothing techniques, and the specific refinements of those to apply them to verb translation.[26] The model which was found to produce the most natural translations of German and French words was a refined alignment model with a first-order dependence and a fertility model[16]. They also provide efficient training algorithms for the models presented, which can give other scientists the ability to improve further on their results. This type of work is specific to computational linguistics, and has applications which could vastly improve understanding of how language is produced and comprehended by computers.\n\nWork has also been done in making computers produce language in a more naturalistic manner. Using linguistic input from humans, algorithms have been constructed which are able to modify a system\\'s style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality.[27] This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural.\n\nText-based interactive approach\nMany of the earliest and simplest models of human-computer interaction, such as ELIZA for example, involve a text-based input from the user to generate a response from the computer. By this method, words typed by a user trigger the computer to recognize specific patterns and reply accordingly, through a process known as keyword spotting.\n\nSpeech-based interactive approach\nRecent technologies have placed more of an emphasis on speech-based interactive systems. These systems, such as Siri of the iOS operating system, operate on a similar pattern-recognizing technique as that of text-based systems, but with the former, the user input is conducted through speech recognition. This branch of linguistics involves the processing of the user\\'s speech as sound waves and the interpreting of the acoustics and language patterns in order for the computer to recognize the input.[28]\n\nComprehension approaches\nMuch of the focus of modern computational linguistics is on comprehension. With the proliferation of the internet and the abundance of easily accessible written human language, the ability to create a program capable of understanding human language would have many broad and exciting possibilities, including improved search engines, automated customer service, and online education.\n\nEarly work in comprehension included applying Bayesian statistics to the task of optical character recognition, as illustrated by Bledsoe and Browing in 1959 in which a large dictionary of possible letters were generated by \\\"learning\\\" from example letters and then the probability that any one of those learned examples matched the new input was combined to make a final decision.[29] Other attempts at applying Bayesian statistics to language analysis included the work of Mosteller and Wallace (1963) in which an analysis of the words used in The Federalist Papers was used to attempt to determine their authorship (concluding that Madison most likely authored the majority of the papers).[30]\n\nIn 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule governed environment. The primary language parsing program in this project was called SHRDLU, which was capable of carrying out a somewhat natural conversation with the user giving it commands, but only within the scope of the toy environment designed for the task. This environment consisted of different shaped and colored blocks, and SHRDLU was capable of interpreting commands such as \\\"Find a block which is taller than the one you are holding and put it into the box.\\\" and asking questions such as \\\"I don\\'t understand which pyramid you mean.\\\" in response to the user\\'s input.[31] While impressive, this kind of natural language processing has proven much more difficult outside the limited scope of the toy environment. Similarly a project developed by NASA called LUNAR was designed to provide answers to naturally written questions about the geological analysis of lunar rocks returned by the Apollo missions.[32] These kinds of problems are referred to as question answering.\n\nInitial attempts at understanding spoken language were based on work done in the 1960s and 1970s in signal modeling where an unknown signal is analyzed to look for patterns and to make predictions based on its history. An initial and somewhat successful approach to applying this kind of signal modeling to language was achieved with the use of hidden Markov models as detailed by Rabiner in 1989.[33] This approach attempts to determine probabilities for the arbitrary number of models that could be being used in generating speech as well as modeling the probabilities for various words generated from each of these possible models. Similar approaches were employed in early speech recognition attempts starting in the late 70s at IBM using word/part-of-speech pair probabilities.[34]\n\nMore recently these kinds of statistical approaches have been applied to more difficult tasks such as topic identification using Bayesian parameter estimation to infer topic probabilities in text documents.[35]\n\nApplications\nModern computational linguistics is often a combination of studies in computer science and programming, math, particularly statistics, language structures, and natural language processing. Combined, these fields most often lead to the development of systems that can recognize speech and perform some task based on that speech. Examples include speech recognition software, such as Apple\\'s Siri feature, spellcheck tools, speech synthesis programs, which are often used to demonstrate pronunciation or help the disabled, and machine translation programs and websites, such as Google Translate.[36]\n\nComputational linguistics can be especially helpful in situations involving social media and the Internet. For example, filters in chatrooms or on website searches require computational linguistics. Chat operators often use filters to identify certain words or phrases and deem them inappropriate so that users cannot submit them.[36] Another example of using filters is on websites. Schools use filters so that websites with certain keywords are blocked from children to view. There are also many programs in which parents use Parental controls to put content filters in place. Computational linguists can also develop programs that group and organize content through Social media mining. An example of this is Twitter, in which programs can group tweets by subject or keywords.[37] Computational linguistics is also used for document retrieval and clustering. When you do an online search, documents and websites are retrieved based on the frequency of unique labels related to what you typed into a search engine. For instance, if you search \\\"red, large, four-wheeled vehicle,\\\" with the intention of finding pictures of a red truck, the search engine will still find the information desired by matching words such as \\\"four-wheeled\\\" with \\\"car\\\".[38]\n\nSubfields\nComputational linguistics can be divided into major areas depending upon the medium of the language being processed, whether spoken or textual; and upon the task being performed, whether analyzing language (recognition) or synthesizing language (generation).\n\nSpeech recognition and speech synthesis deal with how spoken language can be understood or created using computers. Parsing and generation are sub-divisions of computational linguistics dealing respectively with taking language apart and putting it together. Machine translation remains the sub-division of computational linguistics dealing with having computers translate between languages. The possibility of automatic language translation, however, has yet to be realized and remains a notoriously hard branch of computational linguistics.[39]\n\nSome of the areas of research that are studied by computational linguistics include:\n\nComputational complexity of natural language, largely modeled on automata theory, with the application of context-sensitive grammar and linearly bounded Turing machines.\nComputational semantics comprises defining suitable logics for linguistic meaning representation, automatically constructing them and reasoning with them\nComputer-aided corpus linguistics, which has been used since the 1970s as a way to make detailed advances in the field of discourse analysis[40]\nDesign of parsers or chunkers for natural languages\nDesign of taggers like POS-taggers (part-of-speech taggers)\nMachine translation as one of the earliest and most difficult applications of computational linguistics draws on many subfields.\nSimulation and study of language evolution in historical linguistics/glottochronology.\nLegacy\nThe subject of computational linguistics has had a recurring impact on popular culture:\n\nThe 1983 film WarGames features a young computer hacker who interacts with an artificially intelligent supercomputer.[41]\nA 1997 film, Conceiving Ada, focuses on Ada Lovelace, considered one of the first computer scientists, as well as themes of computational linguistics.[42]\nHer, a 2013 film, depicts a man\\'s interactions with the \\\"world\\'s first artificially intelligent operating system.\\\"[43]\nThe 2014 film The Imitation Game follows the life of computer scientist Alan Turing, developer of the Turing Test.[44]\nThe 2015 film Ex Machina centers around human interaction with artificial intelligence.[45]\n\nHere\\'s the distinction I personally make:\n\nComputational linguistics is analogous to computational biology or any other computational fill-in-the-blank. It develops computational methods to answer the scientific questions of linguistics.\n\nThe core questions in linguistics involve the nature of linguistic representations and linguistic knowledge, and how linguistic knowledge is acquired and deployed in the production and comprehension of language. Answering these questions describes the human language ability and may help to explain the distribution of linguistic data and behavior that we actually observe.\n\nIn computational linguistics, we propose formal answers to these core questions. Linguists are really asking what humans are computing and how. So we mathematically define classes of linguistic representations and formal grammars (which are usually probabilistic models nowadays) that seem adequate to capture the range of phenomena in human languages. We study their mathematical properties, and devise efficient algorithms for learning, production, and comprehension. Because the algorithms can actually run, we can test our models and find out whether they make appropriate predictions.\n\nLinguistics also considers a variety of questions beyond this core -- think of sociolinguistics, historical linguistics, psycholinguistics, and neurolinguistics. These scientific questions are fair game as well for computational linguists, who might use models and algorithms to make sense of the data. In this case, we are not trying to model the competence of everyday speakers in their native language, but rather to automate the special kind of reasoning that linguists do, potentially enabling us to work on bigger datasets (or even new kinds of data) and draw more accurate conclusions. Similarly, computational linguists may design software tools to help document endangered languages.\n\nNatural language processing is the art of solving engineering problems that need to analyze (or generate) natural language text. Here, the metric of success is not whether you designed a better scientific theory or proved that languages X and Y were historically related. Rather, the metric is whether you got good solutions on the engineering problem.\n\nFor example, you don\\'t judge Google Translate on whether it captures what translation \\\"truly is\\\" or explains how human translators do their job. You judge it on whether it produces reasonably accurate and fluent translations for people who need to translate certain things in practice. The machine translation community has ways of measuring this, and they focus strongly on improving those scores.\n\nNLP is mainly used to help people navigate and digest large quantities of information that already exist in text form. It is also used to produce better user interfaces so that humans can better communicate with computers and with other humans.\n\nBy saying that NLP is engineering, I don\\'t mean that it is always focused on developing commercial applications. NLP may be used for scientific ends within other academic disciplines such as political science (blog posts), economics (financial news and reports), medicine (doctor\\'s notes), digital humanities (literary works, historical sources), etc. But then it is being used as a tool within computational X-ology in order to answer the scientific questions of X-ologists, rather than the scientific questions of linguists.\n\nBoth fields make use of formal training in CS, linguistics, and machine learning. If you want to truly advance either field in a lasting way, you should develop enough strength to do original research in all three of these areas. It might help to go to a school with a strong interdisciplinary culture, where many of the CS faculty and students are actively interested in linguistics for its own sake (or vice-versa).\n\nThat said, NLP people often get away with relatively superficial linguistics. They look at the errors made by their current system, and learn only as much linguistics as they need to understand and fix the most prominent types of errors. After all, their goal is not a full theory but rather the simplest, most efficient approach that will get the job done.\n\nConversely, if you study computational linguistics in a linguistics department, you will typically get a lot more linguistics and a lot less CS/ML. The students in those departments are technically adept, since linguistics is quite a technical field. But they tend to know much less math and CS. So the computational courses tend to be providing only some exposure to formal language theory, programming, and applied NLP. (These courses are popular among linguistics students who hope to improve their employability.)\n\nEventually I hope the two research programs will draw even closer together. If we can build a strong model of the human linguistic capacity, then that should solve a wide range of NLP problems for us. So today\\'s computational linguistics is developing methods for tomorrow\\'s NLP. That\\'s been historically true too.\n\n", "qas": [{"answers": [{"answer_start": -1, "text": ""}], "question": "What is question answering ?", "id": "What is question answering ?"}]}]}], "version": "my_ver"}